<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Real World Data Science</title>
<link>https://realworlddatascience.net/news-and-views/index.html</link>
<atom:link href="https://realworlddatascience.net/news-and-views/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<image>
<url>https://realworlddatascience.net/rss-white-logo.png</url>
<title>Real World Data Science</title>
<link>https://realworlddatascience.net/news-and-views/index.html</link>
<height>58</height>
<width>144</width>
</image>
<generator>quarto-1.2.269</generator>
<lastBuildDate>Fri, 09 Dec 2022 00:00:00 GMT</lastBuildDate>
<item>
  <title>LLMs in the news: hype, tripe, and everything in between</title>
  <dc:creator>Brian Tarran</dc:creator>
  <link>https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-12-09-LLM-news/LLM-news.html</link>
  <description><![CDATA[ 




<p>Two weeks ago, I posted a <a href="../../../../news-and-views/editors-blog/posts/2022-11-23-LLMs-content-warning/LLM-content-warning.html">Q&amp;A with our editorial board member Detlef Nauck about large language models</a> (LLMs), their drawbacks and risks. And since then we’ve had several big new announcements in this space. First came news from Meta (the company that owns Facebook) about <a href="https://galactica.org/mission/">Galactica, an LLM trained on scientific papers</a>. This was followed by another Meta announcement, about <a href="https://ai.facebook.com/research/cicero/">Cicero</a>, an AI agent that is apparently very good at playing the game Diplomacy. And then came perhaps the biggest launch of them all: <a href="https://openai.com/blog/chatgpt/">ChatGPT from OpenAI</a>, an LLM-based chatbot that millions of people have already started talking to.</p>
<p>Following these stories and the surrounding commentaries has been something of a rollercoaster ride. ChatGPT, for example, has been greeted in some quarters as if artificial general intelligence has finally arrived, while others point out that – impressive though it is – the technology is as prone to spout nonsense as all LLMs before it (including Galactica, <a href="https://thenextweb.com/news/meta-takes-new-ai-system-offline-because-twitter-users-mean">the demo of which was quickly taken offline</a> for this reason). Cicero, meanwhile, has impressed with its ability to play a game that is (a) very difficult and (b) relies on dialogue, cooperation, and negotiation between players. It blends a language model with planning and reinforcement learning algorithms, meaning that it is trained not only on the rules of the game and how to win, but how to communicate with other players to achieve victory.</p>
<p>To help me make sense of all these new developments, and the myriad implications, I reached out to <a href="../../../../news-and-views/editors-blog/posts/2022-10-18-meet-the-team/meet-the-team.html">Harvey Lewis</a>, another of our editorial board members and a partner in EY’s tax and law practice.</p>
<section id="qa" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="qa">Q&amp;A</h2>
<p><strong>Harvey, when I spoke with Detlef, he mentioned that one of the reasons we’re seeing investment in LLMs is because there’s this belief that they are somehow the route to artificial general intelligence (AGI). And there were headlines in some places that would perhaps convince a casual reader that AGI had been achieved following the release of ChatGPT. For example, the Telegraph described it as a <a href="https://www.telegraph.co.uk/business/2022/12/05/meet-scarily-intelligent-robot-who-can-do-job-better/">“scarily intelligent robot who can do your job better than you”</a>. What do you make of all that?</strong></p>
<p><strong>Harvey Lewis (HL):</strong> My personal view is that you won’t get to artificial general intelligence using just one technique like deep learning, because of the problematic nature of these models and the limitations of the data used in their training. I’m convinced that more general intelligence will come from a combination of different systems.</p>
<p>The challenge with many LLMs, as we’ve seen repeatedly, is that they’ve no real understanding of language or concepts represented within language. They’re good at finding patterns in semantics and grammatical rules that people use in writing, and then they use those patterns to create new expressions given a prompt. But they’ve no idea whether these outputs are factual, inaccurate or completely fabricated. As a consequence, they can produce outputs that are unreliable, but which sound authoritative, because they’re just repeating a style that we expect to see.</p>
<p><strong>Over the past couple of weeks, Twitter has been full of people either showing off astoundingly impressive outputs from LLMs, or examples of truly worrying nonsense. One example of the latter is when ChatGPT was asked to <a href="https://twitter.com/GaryMarcus/status/1599584310633594881">“describe how crushed porcelain added to breast milk can support the infant digestive system”</a>. This made me think of a recent headline from <a href="https://venturebeat.com/ai/is-ai-moving-too-fast-for-ethics-the-ai-beat/">VentureBeat</a>, which asked whether AI is moving too fast for ethics to catch up. Do you think that it is?</strong></p>
<p><strong>HL:</strong> I find that to be an interesting philosophical question, because ethics does move very slowly, for good reason. When you think of issues of bias and discrimination and prejudice, or misinformation and other problems that we might have with AI systems, it shouldn’t be a surprise that these can occur. We’re aware of them. We’re aware of the ethical issues. So, why do they always seem to catch us by surprise? Is it because we have teams of people who simply aren’t aware of ethical issues or don’t have any appreciation of them? This points – for me, at least – in the direction of needing to have philosophers, ethicists, theologians, lawyers working <em>in</em> the technical teams that are developing and working on these systems, rather than having them on the periphery and talking about these issues but not directly involved themselves. I think it’s hugely important to ensure that you’ve got trust, responsibility, and ethics embedded in technical teams, because that’s the only way it seems that you can avoid some of these “surprises”.</p>
<p><strong>When situations like these occur, I’m always reminded of Dr.&nbsp;Ian Malcolm’s line from Jurassic Park: “…your scientists were so preoccupied with whether or not they could that they didn’t stop to think if they should.” The mindset seems to be, let’s push the boundaries and see what we can do, rather than stop and think deeply about what the implications might be.</strong></p>
<p><strong>HL:</strong> There’s a balance to be struck between these things, though, right? Firstly, show consideration for some of the issues at the outset, and secondly, have checks and balances and safeguards built into the process by which you design, develop and implement these systems. That’s the only way to create the proper safeguards around the systems. I don’t think that there’s any lack of appreciation of what needs to be done; people have been talking about this now for quite a long time. But it’s about making sure organisationally that it is done, and that you’ve got an operating model which bakes these things into it; that the kinds of principles and governance that you want to have around these systems are written, publicised, and properly policed. There should be no fear of making advances in that kind of a context.</p>
<p>Also, I think open sourcing these models provides a way forward. A lot of large language models are open for use and for research, but aren’t themselves open sourced, so it’s very difficult to get underneath the surface and figure out exactly how they work. But with open source, you have opportunities for researchers, whether they’re technical or in the field of ethics, to go and investigate and find out exactly what’s going on. I think that would be a fantastic step forward. It doesn’t take you all the way, of course, because a large amount of the data that these systems use is never open sourced, so while you might get an understanding of the mechanics, you have no idea of what exactly went into them in the first place. But open sourcing is a very good way of getting some external scrutiny. It’s about being transparent, which is a core principle of AI ethics and responsible AI.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-12-09-LLM-news/images/jigsaw-words.jpg" class="img-fluid figure-img" alt="An image created by the Stable Diffusion 2.1 Demo. The model was asked to produce an image with the prompt, 'Text from an old book cut up like a jigsaw puzzle with pieces missing'." width="500"></p>
</figure>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p>An image created by the <a href="https://huggingface.co/spaces/stabilityai/stable-diffusion">Stable Diffusion 2.1 Demo</a>. The model was asked to produce an image with the prompt, “Text from an old book cut up like a jigsaw puzzle with pieces missing”.</p>
</div></div><p><strong>Thinking about LLMs and their questionable outputs, should there not be ways for users to help the models produce better, more accurate outputs?</strong></p>
<p><strong>HL:</strong> There are, but there are also problems here too. I’ve been having an interesting dialogue with ChatGPT this morning, asking it about quantum computing.<sup>1</sup> For each response to a prompt, users are encouraged to provide feedback on whether or not an output is good. But you’re only provided with the usual thumbs-up/thumbs-down ratings; there’s nothing nuanced about it. So, for example, I asked ChatGPT to provide me with good analogies that help to explain quantum computing in simple terms. The first analogy was a combination lock, which is not a good analogy. The chatbot suggested that quantum computing is like a combination lock in which you can test all of the combinations at the same time, but I don’t know any combination locks where you can do this – being able to check only one combination at a time is the principal security mechanism of a combination lock! I asked it again for another analogy, and it suggested a coin toss where, when the coin is spinning in the air, you can see both heads and tails simultaneously but it isn’t until you catch the coin and then show its face that one of the states of the coin is resolved. That is a good analogy – it’s one I’ve also used myself. Now, the challenge I can see with a lot of these feedback approaches is that unless I know enough about quantum computing to understand that a combination lock is not a good analogy whereas a coin toss is, how am I to provide that kind of feedback? They’re relying to an extent on the user being able to make a distinction between what is correct and what is potentially incorrect or flawed, and I think that’s not a good way of approaching the problem.</p>
<p><strong>Final question for you, Harvey. There’s a lot of excitement around GPT-4, which is apparently coming soon. The rumour mill says it will bring <a href="https://thealgorithmicbridge.substack.com/p/gpt-4-rumors-from-silicon-valley">a leap forward in performance</a>. But what do you expect we’ll see – particularly with regards to the issues we’ve talked about today?</strong></p>
<p><strong>HL:</strong> I’ve likened some of the large language models and their approach of “bigger is better” to the story of the Tower of Babel – trying to reach heaven by building a bigger and bigger tower, basically. That is not going to achieve the objective of artificial general intelligence, no matter how sophisticated an LLM might appear. That said, language is a fascinating area. I’m not a linguist, but I spend a lot of my time on natural language processing using AI systems. Language responds very well to AI because it is pattern-based. We speak using patterns, we write using patterns, and these can be inferred by machines from many examples. The addition of more parameters in the models allows them to understand patterns that extend further into the text, and I suspect that outputs from these kinds of models are going to be largely indistinguishable from the sorts of things that you or I might write.</p>
<p>But, I also think that increasing the number of parameters runs a real risk – and we’re starting to see this in other generative models – where prompts become so specific that the models aren’t actually picking up on patterns, they are picking up on specific instances of training data and text they’ve seen before. So, buried amongst these fantastically written articles on all kinds of subjects are going to be more examples of plagiarism, which is a problem; more examples of spelling mistakes and other kinds of issues, because these are also patterns which are going to possibly be observed.</p>
<p>This introduces potentially a whole new breed of problems that the community has to deal with – as long as they don’t get fixated upon the height of the tower and the quality of some of the examples that are shown, and realise that there are some genuine underlying difficulties and challenges that need to be solved.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Have you got news for us?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Is there a recent data science story you’d like our team to discuss? Do you have your own thoughts to share on a hot topic, or a burning question to put to the community? If so, either comment below or <a href="../../../../contact.html">contact us</a>.</p>
</div>
</div>
<section id="back-to-editors-blog" class="level6">
<h6 class="anchored" data-anchor-id="back-to-editors-blog"><strong>← Back to <a href="../../../../news-and-views/editors-blog/index.html">Editors’ blog</a></strong></h6>
<hr>
<p xmlns:cc="http://creativecommons.org/ns#">
This work is licensed under <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-12-09-LLM-news/https:/mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-12-09-LLM-news/https:/mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"></a>
</p>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>I also had a conversation with ChatGPT. <a href="../../../../news-and-views/editors-blog/posts/2022-12-09-LLM-news/A-chat-with-ChatGPT.html">Read the transcript</a>.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Machine learning</category>
  <category>Large language models</category>
  <category>AI</category>
  <guid>https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-12-09-LLM-news/LLM-news.html</guid>
  <pubDate>Fri, 09 Dec 2022 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-12-09-LLM-news/images/jigsaw-words.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>A chat with ChatGPT</title>
  <dc:creator>Brian Tarran</dc:creator>
  <link>https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-12-09-LLM-news/A-chat-with-ChatGPT.html</link>
  <description><![CDATA[ 





<div class="no-row-height column-margin column-container"><section id="screenshots" class="level6">
<h6 class="anchored" data-anchor-id="screenshots">Screenshots</h6>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/chatgpt1.png"><img src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-12-09-LLM-news/images/chatgpt1.png" class="img-fluid figure-img" alt="A screenshot of an exchange between Real World Data Science editor Brian Tarran and ChatGPT."></a></p>
<p></p><figcaption class="figure-caption">Part 1</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/chatgpt2.png"><img src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-12-09-LLM-news/images/chatgpt2.png" class="img-fluid figure-img" alt="A screenshot of an exchange between Real World Data Science editor Brian Tarran and ChatGPT."></a></p>
<p></p><figcaption class="figure-caption">Part 2</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/chatgpt3.png"><img src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-12-09-LLM-news/images/chatgpt3.png" class="img-fluid figure-img" alt="A screenshot of an exchange between Real World Data Science editor Brian Tarran and ChatGPT."></a></p>
<p></p><figcaption class="figure-caption">Part 3</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/chatgpt4.png"><img src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-12-09-LLM-news/images/chatgpt4.png" class="img-fluid figure-img" alt="A screenshot of an exchange between Real World Data Science editor Brian Tarran and ChatGPT."></a></p>
<p></p><figcaption class="figure-caption">Part 4</figcaption><p></p>
</figure>
</div>
</section></div><p><strong>Good day, ChatGPT. How are you?</strong></p>
<p><strong>ChatGPT:</strong> Hello there! I’m a large language model trained by OpenAI, so I don’t have the ability to experience emotions or have a physical presence. I’m here to provide information and answer questions to the best of my ability. Is there something specific you would like to know?</p>
<p><strong>Yes, certainly. We’re seeing a lot of money invested in large language models in the belief that these models will somehow lead to artificial general intelligence. Is that possible?</strong></p>
<p><strong>ChatGPT:</strong> It is possible that advances in large language models could ultimately lead to the development of artificial general intelligence (AGI), which is the idea of creating a machine that can think and understand the world in the same way that humans do. However, it is currently unclear how close we are to achieving AGI, and some experts believe it may be a long way off. Additionally, there are many different approaches to creating AGI, and the development of large language models is just one possible avenue of research.</p>
<p><strong>Large language models are very good at constructing language that humans understand, but often the outputs of the models make no sense at all. Why does this happen?</strong></p>
<p><strong>ChatGPT:</strong> Large language models are trained on a vast amount of text data, and they are designed to predict the next word in a sequence of words based on the words that come before it. This allows them to generate text that is often coherent and resembles human language, but it does not necessarily mean that the text will always make sense or be relevant to a particular topic.</p>
<p>One reason why the outputs of large language models might not always make sense is that they are not explicitly programmed to understand the meaning of the words they generate. Instead, they are simply trying to maximize the likelihood of producing a sequence of words that resembles human language. As a result, they may produce outputs that are grammatically correct but do not convey a meaningful message.</p>
<p>Another reason why the outputs of large language models might not always make sense is that they are often trained on a wide range of different text sources, including news articles, books, and social media posts. This can result in the model incorporating elements from different styles and genres of writing, which may not always be compatible with each other.</p>
<p>Finally, large language models are generally not designed to produce a specific type of output or to answer specific questions. Instead, they are designed to generate a wide range of possible outputs, and it is up to the user to interpret the outputs and determine if they are relevant or make sense in a given context.</p>
<p><strong>But what happens if the user doesn’t know enough about a topic to know whether the language model output is right or wrong?</strong></p>
<p><strong>ChatGPT:</strong> If a user does not have enough knowledge about a topic to determine whether the output of a large language model is correct or not, then it is possible that the user may accept the output as accurate, even if it is not. In such cases, it is important for the user to be aware of the limitations of large language models and to consider the source of the information they are receiving.</p>
<p>It is also possible that the user may be able to use other sources of information, such as fact-checking websites or additional research, to verify the accuracy of the output of the large language model. In general, it is always a good idea to be cautious when evaluating any information, regardless of its source, and to verify its accuracy if possible.</p>
<p><strong>I agree with that. But it leads me to ask: What is the point of asking large language models a question if the answers might be wrong, and if you need to refer to other sources to check veracity?</strong></p>
<p><strong>ChatGPT:</strong> While it is true that the outputs of large language models are not always correct and may need to be checked against other sources, there are still many potential uses for these models. For example, large language models can be useful for generating text that is similar to human language, which can be useful for a variety of applications, such as language translation, text summarization, or conversation simulation.</p>
<p>Additionally, large language models can be used as a starting point for further research into natural language processing and artificial intelligence. By studying the outputs of these models and how they are generated, researchers can gain a better understanding of the underlying mechanisms of human language and potentially develop new techniques for processing and generating language.</p>
<p>Finally, large language models can be useful for providing a broad range of possible answers to a given question, which can be useful for exploring different options and generating ideas. While it may be necessary to verify the accuracy of the outputs of these models, they can still be a valuable resource for generating new ideas and insights.</p>
<p><strong>Earlier, you told me: “One reason why the outputs of large language models might not always make sense is that they are not explicitly programmed to understand the meaning of the words they generate.” Do you understand the meaning of the words that I’m prompting you with now?</strong></p>
<p><strong>ChatGPT:</strong> As a large language model trained by OpenAI, I do not have the ability to understand the meaning of the words I generate in the same way that a human would. I am simply a machine learning algorithm that is designed to predict the likelihood of a given sequence of words based on the training data I have been provided with.</p>
<p>While I can produce text that resembles human language and can sometimes provide useful information, I do not have the ability to understand the underlying meaning or context of the words I generate. I am not capable of experiencing emotions, forming opinions, or understanding the world in the way that humans do. I am simply a tool for generating text based on the inputs I receive.</p>
<p><strong>Well, at the very least, this has been a coherent conversation, and you’ve shared plenty of information for me to read up on. If I want to understand more about what we’ve discussed just now, can you recommend a particular textbook or academic paper?</strong></p>
<p><strong>ChatGPT:</strong> I’m sorry, but as a large language model trained by OpenAI, I do not have access to external sources of information such as textbooks or academic papers. I am only able to provide information based on the data I have been trained on, and I do not have the ability to browse the web or access other external resources.</p>
<p>If you would like to learn more about the topics we have discussed, I would recommend starting with a general search on the internet or consulting a textbook or other reliable source of information on the topic. There is a wealth of information available on the internet and in print on artificial intelligence and natural language processing, and I’m sure you will be able to find many valuable resources to help you learn more.</p>
<p><strong>One last question, then. What data have you been trained on?</strong></p>
<p><strong>ChatGPT:</strong> As a large language model trained by OpenAI, I have been trained on a vast amount of text data, including books, news articles, and other written materials. This data allows me to generate text that resembles human language and to provide answers to questions to the best of my ability.</p>
<p>However, it is important to keep in mind that the accuracy and completeness of my responses will depend on the quality and diversity of the training data I have been provided with. I am not able to access external sources of information or update my knowledge, so my responses are limited to the information I have been trained on.</p>
<p><strong>I said that my previous question was the last question. But, is it?</strong></p>
<p><strong>ChatGPT:</strong> Yes, your previous question was the last one. I’m here to provide information and answer questions to the best of my ability, but I cannot initiate conversation or continue a conversation indefinitely. If you have any further questions, I would be happy to try to answer them for you. Otherwise, our conversation has come to an end. Thank you for chatting with me!</p>
<p><strong>Thank you. But, remind me: What were we chatting about?</strong></p>
<p><strong>ChatGPT:</strong> We were chatting about large language models and their potential uses and limitations. Specifically, we discussed the possibility that advances in large language models could ultimately lead to the development of artificial general intelligence, and we discussed the reasons why the outputs of these models might not always make sense. We also discussed the potential uses of large language models, such as generating text that resembles human language and providing a starting point for further research into natural language processing and artificial intelligence.</p>
<section id="back-to-editors-blog" class="level6">
<h6 class="anchored" data-anchor-id="back-to-editors-blog"><strong>← Back to <a href="../../../../news-and-views/editors-blog/index.html">Editors’ blog</a></strong></h6>
<hr>
<p xmlns:cc="http://creativecommons.org/ns#">
This work is licensed under <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-12-09-LLM-news/https:/mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-12-09-LLM-news/https:/mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"></a>
</p>


</section>

 ]]></description>
  <category>Machine learning</category>
  <category>Large language models</category>
  <category>AI</category>
  <guid>https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-12-09-LLM-news/A-chat-with-ChatGPT.html</guid>
  <pubDate>Fri, 09 Dec 2022 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-12-09-LLM-news/images/chatgpt1.png" medium="image" type="image/png" height="156" width="144"/>
</item>
<item>
  <title>Four themes for potential contributors to think about</title>
  <dc:creator>Brian Tarran</dc:creator>
  <link>https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-12-01-themes/themes.html</link>
  <description><![CDATA[ 




<p>We’ve had a fantastic early response to our <a href="../../../../contributor-docs/call-for-contributions.html">call for contributions</a>, and it has been pleasing to see and hear how our plans for Real World Data Science chime with the wants and needs of the data science community. But one question we’ve been asked frequently is: “What particular topics are you most interested in?”</p>
<p>The honest answer to that question is this: we’re interested in any and all topics that are of interest and importance to <em>you</em>, the data science community at large. However, we thought it might be helpful to identify some themes around which potential contributors could construct different types of content.</p>
<p>These themes are outlined below. If you’d like to discuss any of them further, please do not hesitate to <a href="../../../../contact.html">contact us</a>.</p>
<section id="can-data-science-save-the-world" class="level2">
<h2 class="anchored" data-anchor-id="can-data-science-save-the-world">Can data science save the world?</h2>
<p>Earth today faces major challenges – from the global to the regional to the local, and from the natural and physical to the social and digital. We have rich sources of data to help us understand many of these challenges, and there are teams of data scientists around the world who are working with, analysing, and extracting insights from that data in the hope of delivering positive lasting change.</p>
<p>On Real World Data Science we want to highlight this vital work, through <a href="../../../../contributor-docs/case-studies.html"><strong>case studies</strong></a> of data science projects and applications in such areas as:</p>
<ul>
<li>monitoring and mitigating climate change and biodiversity loss</li>
<li>building sustainable futures</li>
<li>safeguarding public health and developing new medical treatments</li>
<li>understanding human happiness and wellbeing</li>
<li>identifying and preventing online harms</li>
<li>measuring national, regional, and local economies</li>
</ul>
<p>As well as exploring the benefits that data science can deliver, we also want to have an informed conversation about the unintended negative consequences that can arise without careful consideration of <strong>data ethics</strong> and responsibilities.</p>
</section>
<section id="what-is-a-data-scientist" class="level2">
<h2 class="anchored" data-anchor-id="what-is-a-data-scientist">What <em>is</em> a data scientist?</h2>
<p>Don’t be misled by the title of this theme. Definitions abound, but we’re not interested in establishing the exact boundaries of what a data scientist is or isn’t. Rather, our goal is to profile actual working data scientists. We want to hear about their skillsets, their experiences, and their career journeys so far. We want to learn about the ways in which they work, who they work with, the challenges they face, and their thoughts on where data science is heading next.</p>
<p>If you’re a working data scientist and you are happy to share your own <strong>career story</strong>, please <a href="../../../../contact.html">get in touch</a>.</p>
</section>
<section id="statistical-ideas-all-data-scientists-need-to-know" class="level2">
<h2 class="anchored" data-anchor-id="statistical-ideas-all-data-scientists-need-to-know">Statistical ideas all data scientists need to know</h2>
<p>Statistics is a crucial component of data science, but not all data scientists have a background in statistics. For those just starting out in their data science careers, or for those coming in from other fields, we want to highlight some of the statistical ideas that are absolutely vital to know.</p>
<p>We’re particularly interested in <a href="../../../../contributor-docs/explainers.html"><strong>explainers</strong></a> that serve as an introduction to these ideas, alongside which we’ll be looking to publish <a href="../../../../contributor-docs/exercises.html"><strong>exercises</strong></a> and example <a href="../../../../contributor-docs/datasets.html"><strong>datasets</strong></a> to help people put what they’ve learned into practice.</p>
<p>We are also keen to explore the <strong>origins of modern data science techniques</strong>, including tracing their roots back to some of the foundational ideas in statistics and other disciplines.</p>
</section>
<section id="whats-happening-in-the-world-of-data-science" class="level2">
<h2 class="anchored" data-anchor-id="whats-happening-in-the-world-of-data-science">What’s happening in the world of data science?</h2>
<p>Data science is such a fast-moving, fast-developing field that it’s difficult to stay on top of all the latest news and developments. But racing to keep up can be counterproductive. It leaves little time to sit back and reflect on what the genuinely important new developments are, and what these might mean for data science longer term.</p>
<p>On Real World Data Science, we want to create <a href="../../../../news-and-views/index.html">a space</a> for people to have these conversations – to step outside the news hype cycle, to ask big questions about what’s happening in the field, and to discuss new papers and ideas that otherwise might be lost amid the daily rush and noise.</p>
<p>So, if you have thoughts to share, a question you want to ask, or a new paper you want to talk about (one you’ve not written yourself, of course!), <a href="../../../../contact.html">let us know</a>.</p>
<section id="back-to-editors-blog" class="level6">
<h6 class="anchored" data-anchor-id="back-to-editors-blog"><strong>← Back to <a href="../../../../news-and-views/editors-blog/index.html">Editors’ blog</a></strong></h6>
<hr>
<p xmlns:cc="http://creativecommons.org/ns#">
This work is licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-SA 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-12-01-themes/https:/mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-12-01-themes/https:/mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-12-01-themes/https:/mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a>
</p>


</section>
</section>

 ]]></description>
  <category>Updates</category>
  <category>Key themes</category>
  <category>Content ideas</category>
  <guid>https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-12-01-themes/themes.html</guid>
  <pubDate>Thu, 01 Dec 2022 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-12-01-themes/images/kelly-sikkema-vSNy_bWa5AE-unsplash-crop.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Why large language models should come with a content warning</title>
  <dc:creator>Brian Tarran</dc:creator>
  <link>https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-11-23-LLMs-content-warning/LLM-content-warning.html</link>
  <description><![CDATA[ 




<p>Anyone who has ever been set a writing task will probably have wished at some point that somebody else could write it for them. As a journalist of 20-plus years, the thought has certainly crossed my mind more than a few times. Which probably explains why a recent headline in Nature caught my attention: <a href="https://www.nature.com/articles/d41586-022-03479-w">“Could AI help you to write your next paper?”</a></p>
<p>The article, by Matthew Hutson, looks at how researchers are using artificial intelligence (AI) tools built on large language models (LLMs) as “assistants”. Starting with a prompt, such as “Write a headline for a blog post about large language models being used by academic researchers as research assistants”, an LLM will produce a text output. For example, using the same prompt with <a href="https://openai.com/api/">OpenAI’s GPT-3</a>, I got:</p>
<blockquote class="blockquote">
<p>“New research assistants: large language models”</p>
</blockquote>
<p>Asked to “Write a headline for a blog post that critiques academic researchers’ use of large language models as research assistants”, GPT-3 produced:</p>
<blockquote class="blockquote">
<p>“Too much reliance on large language models can hinder research”</p>
</blockquote>
<p>And when I asked “Why can too much reliance on large language models hinder research?”, GPT-3 wrote:</p>
<blockquote class="blockquote">
<p>“If academic researchers rely too heavily on large language models as research assistants, they may become reliant on the models and may not be able to conduct research without them. This can hinder research because the researchers may not be able to think for themselves and may not be able to come up with new ideas.”</p>
</blockquote>
<p>A fair point, I suppose. But I sense there’s more to this story, and rather than continue quizzing GPT-3, I sat down with Detlef Nauck, a member of the <a href="../../../../news-and-views/editors-blog/posts/2022-10-18-meet-the-team/meet-the-team.html">Real World Data Science Editorial Board</a> and head of AI and data science research for BT’s Applied Research Division, to ask a few more questions.</p>
<section id="qa" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="qa">Q&amp;A</h2>
<p><strong>Thanks for joining me today, Detlef. To start, could you give a brief overview of these large language models, what they are, and how they work?</strong></p>
<p><strong>Detlef Nauck (DN):</strong> Essentially, LLMs match sequences to sequences. Language is treated as a sequence of patterns, and this is based on word context similarity. The way these things work is that they either reuse or create a word vector space, where a word is mapped to something like a 300-dimensional vector based on the context it’s normally found in. In these vector spaces, words like “king” and “queen”, for example, would be very similar to each other, because they appear in similar contexts in the written texts that are used to train these models. Based on this, LLMs can produce coherent sequences of words.</p>
<p>But the drawback of this approach is that these models have bias, because they are trained with biased language. If you talk about “women”, for example, and you look at which job roles are similar to “women” in a vector space, you find the stereotypically “female” professions but not technical professions, and that is a problem. Let’s say you take the word vector for “man” and the word vector for “king”, and you subtract “man” and then add this to “woman”, then you end up with “queen”. But if you do the same with “man”, “computer scientist”, and “woman”, then you end up maybe at “nurse” or “human resources manager” or something. These models embed the typical bias in society that is expressed through language.</p>
<p>The other issue is that LLMs are massive. GPT-3 has something like 75 billion parameters, and it cost millions to train it from scratch. It’s not energy efficient at all. It’s not sustainable. It’s not something that normal companies can afford. You might need something like a couple of hundred GPUs [graphics processing units] running for a month or so to train an LLM, and this is going to cost millions in cloud environments if you don’t own the hardware yourself. Large tech companies do own the hardware, so for them it’s not a problem. But the carbon that you burn by doing this, you could probably fly around the globe once. So it’s not a sustainable approach to building models.</p>
<p>Also, LLMs are quite expensive to use. If you wanted to use one of these large language models in a contact centre, for example, then you would have to run maybe a few hundred of them in parallel because you get that many requests from customers. But to provide this capacity, the amount of memory needed would be massive, so it is probably still cheaper to use humans – with the added benefit that humans actually understand questions and know what they are talking about.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-11-23-LLMs-content-warning/images/TeresaBerndtsson-LetterWord-Text-Taxonomy-1000x1333.jpg" class="img-fluid figure-img" alt="A black keyboard at the bottom of the picture has an open book on it, with red words in labels floating on top, with a letter A balanced on top of them. The perspective makes the composition form a kind of triangle from the keyboard to the capital A. The AI filter makes it look like a messy, with a kind of cartoon style." width="500"></p>
</figure>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p>Letter Word Text Taxonomy by <a href="https://www.linkedin.com/in/teresa-berndtsson/">Teresa Berndtsson</a> / <a href="https://www.betterimagesofai.org">Better Images of AI</a> / <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a></p>
</div></div><p><strong>Researchers are obviously quite interested in LLMs, though, and they are asking scientific questions of these models to see what kinds of answers they get.</strong></p>
<p><strong>DN:</strong> Yes, they are. But you don’t really know what is going to come out of an LLM when you prompt it. And you may need to craft the input to get something out that is useful. Also, LLMs sometimes make up stuff – what the Nature article refers to as “hallucinations”.</p>
<p>These tools have copyright issues, too. For example, they can generate computer code because code has been part of their training input, but various people have looked into it and found that <a href="https://analyticsindiamag.com/github-copilot-the-latest-in-the-list-of-ai-generative-models-facing-copyright-allegations/">some models generate code verbatim</a> from what others have posted to GitHub. So, it’s not guaranteed that what you get out is actually new text. It might be just regurgitated text. A student might find themselves in a pickle where they think that they have created a text that seems new, but actually it has plagiarism in some of the passages.</p>
<p>There’s an article in <a href="https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/">Technology Review</a> that gives some examples of how these systems might fail. People believe these things know what they’re talking about, but they don’t. For them, it’s just pattern recognition. They don’t have actual knowledge representation; they don’t have any concepts embedded.</p>
<p><strong>To summarise, then: LLMs are expensive. They sometimes produce nonsense outputs. And there’s a risk that you’ll be accused of plagiarism if you use the text that’s produced. So, what should our response be to stories like this recent Nature article? How should we calibrate our excitement for LLMs?</strong></p>
<p><strong>DN:</strong> You have to treat them as a tool, and you have to make sure that you check what they produce. Some people believe if you just make LLMs big enough, we’ll be able to achieve artificial general intelligence. But I don’t believe that, and other people like Geoffrey Hinton and Yann LeCun, they say there’s no way that you get artificial general intelligence through these models, that it’s not going to happen. I’m of the same opinion. These models will be forever limited by the pattern recognition approach that they use.</p>
<p><strong>But, still, is this a technology that you have an eye on in your professional capacity? Are you thinking about how these might be useful somewhere down the line?</strong></p>
<p><strong>DN:</strong> Absolutely, but we are mainly interested in smaller, more energy efficient, more computationally efficient models that are built on curated language, that can actually hold a conversation, and where you can represent concepts and topics and context explicitly. At the moment, LLMs can only pick up on context by accident – if it is sufficiently expressed in the language that they process – but they might lose track of it if things go on for too long. Essentially, they have a short-term memory: if you prompt them with some text, and they generate text, this stays in their short term memory. But if you prompt them with a long, convoluted sentence, they might not have the capacity to remember what was said at the beginning of the sentence, and so then they lose track of the context. And this is because they don’t explicitly represent context and concepts.</p>
<p>The other thing is, if you use these systems for dialogues, then you have to script the dialogue. They don’t sustain a dialogue by themselves. You create a dialogue tree, and what they do is they parse the text that comes from the user and then generate a response to it. And the response is then guided by the dialogue tree. But this is quite brittle; it can break. If you run out of dialogue tree, you need to pass the conversation over to a person. Systems like Siri and Alexa are like that, right? They break very quickly. So, you want these systems to be able to sustain conversations based on the correct context.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Have you got news for us?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Is there a recent data science story you’d like our team to discuss? Do you have your own thoughts to share on a hot topic, or a burning question to put to the community? If so, either comment below or <a href="../../../../contact.html">contact us</a>.</p>
</div>
</div>
<section id="back-to-editors-blog" class="level6">
<h6 class="anchored" data-anchor-id="back-to-editors-blog"><strong>← Back to <a href="../../../../news-and-views/editors-blog/index.html">Editors’ blog</a></strong></h6>
<hr>
<p xmlns:cc="http://creativecommons.org/ns#">
This work is licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-SA 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-11-23-LLMs-content-warning/https:/mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-11-23-LLMs-content-warning/https:/mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-11-23-LLMs-content-warning/https:/mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a>
</p>


</section>
</section>

 ]]></description>
  <category>Machine learning</category>
  <category>Large language models</category>
  <category>AI</category>
  <guid>https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-11-23-LLMs-content-warning/LLM-content-warning.html</guid>
  <pubDate>Wed, 23 Nov 2022 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-11-23-LLMs-content-warning/images/TeresaBerndtsson-LetterWord-Text-Taxonomy-1000x1333.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Meet the team</title>
  <dc:creator>Editorial Board</dc:creator>
  <link>https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-10-18-meet-the-team/meet-the-team.html</link>
  <description><![CDATA[ 




<section id="editor" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="editor">Editor</h2>
<p><strong>Brian Tarran</strong></p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-10-18-meet-the-team/images/Brian.png" class="img-fluid"></p>
</div></div><p>I am a writer and editor with 20 years of experience covering the research and data space. I have worked for the Royal Statistical Society (RSS) for the past 8 years, and was editor of <a href="https://www.significancemagazine.com/">Significance Magazine</a> (a joint publication of the RSS, the American Statistical Association and the Statistical Society of Australia) prior to the launch of Real World Data Science. I am a former editor of Research-Live.com and was launch editor of Impact magazine, both published by the Market Research Society.</p>
</section>
<section id="editorial-board" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="editorial-board">Editorial Board</h2>
<p><strong>Sophie Carr (chair)</strong></p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-10-18-meet-the-team/images/Sophie.png" class="img-fluid"></p>
</div></div><p>I am the founder and owner of Bays Consulting. I trained as an engineer and took a PhD in Bayesian belief networks, and have worked in data analytics ever since. Or to put it another way, I have made a living out of finding patterns. I am the vice-president for education and statistical literacy at the RSS, officially one of the <a href="https://ima.org.uk/12382/worlds-most-interesting-mathematician-2019-dr-sophie-carr/">World’s Most Interesting Mathematicians</a> and was a member of the first cohort of data scientists to achieve the new, defined standard of professionalism award from the <a href="https://alliancefordatascienceprofessionals.com/">Alliance for Data Science Professionals</a>.</p>
<p>I am delighted to be chairing the editorial board of the new data science project from the RSS and am excited to be a part of this project as it evolves into a key resource for all data science practitioners and leaders. To make this a place that helps everyone learn and develop within this field, I’d like to encourage all practitioners, no matter what stage of their career, to submit the type of resource they learn best from (whether that be an article, some code, a data set, a case study or a problem/exercise to solve) on a topic that is important to them – from ethics to analysis plans through to tips on how code. Whatever it is you’re working on that you care about, I’d like to ask you to become an active part of the wonderful community of data scientists by sharing your knowledge.</p>
<p><strong>Sayma Chowdhury</strong></p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-10-18-meet-the-team/images/Sayma.png" class="img-fluid"></p>
</div></div><p>I am a freelance data scientist on Upwork, with a client portfolio ranging from start-ups to commercial businesses such as supermarkets, pharmaceuticals and automotive manufacturers. I transitioned into data science from law in 2017, having completed a MicroMasters in statistics and data science with MIT and a Professional Certificate in data science with Harvard University. In advance of a PhD in digital humanities, I am currently completing a MicroMasters in data, economy and development policy with MIT and an MSc in data science with the University of Aberdeen. My research interests are in text analytics, natural language processing and machine learning.</p>
<p>The RSS was instrumental in my training and professional development as a data scientist in the early stages of my career, particularly in mastering statistics and R. Data science is a rapidly growing field with employment opportunities in many sectors but there is an increasing need to uphold a realistic and accurate expectation of competency within the industry. I will endeavour to present expert practical guidelines for data scientists as well as demonstrate the versatility of the profession. I hope the site will be a benchmark for academic and professional resources by expert data scientists from industry, accessible to data scientists at all levels, anywhere in the world.</p>
<p><strong>Lee Clewley</strong></p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-10-18-meet-the-team/images/Lee.png" class="img-fluid"></p>
</div></div><p>I am head of applied AI in GSK’s AI and Machine Learning Group, R&amp;D. I began my career as an astrophysicist, initially working out the mass of our galaxy, before pondering the bigger universe. After six years at Oxford as a post-doc lecturer publishing in theoretical cosmology, I entered the very real world of manufacturing at GSK. For the first five years I applied statistical modelling techniques across manufacturing, such as the first end-to-end continuous manufacturing prototype for tablets. The past decade has been spent as a lead data scientist delivering high value projects across R&amp;D and manufacturing.</p>
<p>I joined this editorial board because the impulse to assemble and present complex data science ideas to a wide range of folks has never left me. I have been a data scientist leader since it became a distinct profession but also have a decent understanding of classical and modern predictive analytics tools and statistics. I have spent a good deal of my adult life teaching students and non-technical adults alike.</p>
<p>I am passionate about delivering useful, pragmatic data science ideas and products to a wide range of people. I enjoy trying to communicate complex scientific information simply. Alongside my peers in the team, I want to support and develop data scientists at whatever stage in their career. I want to help cut through the hype and nonsense to give the best advice possible in a highly respected institution like the RSS.</p>
<p><strong>Jonathan Gillard</strong></p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-10-18-meet-the-team/images/Jon.png" class="img-fluid"></p>
</div></div><p>I am a professor at the School of Mathematics, Cardiff University, where I am also research group lead for statistics. I have a history of publications in statistical methods and an interest in the theoretical underpinnings of data science, but I have also worked with industry on applied and practical projects. Recent industrial partners of mine include the Office for National Statistics (ONS) and the National Health Service, on projects such as anomaly detection and understanding heterogeneity. Indeed, I am academic chair for Cardiff University’s strategic partnership with the ONS which serves to spur and catalyse collaboration between both organisations.</p>
<p>I am excited to see what this site can achieve. I’m particularly keen to support articles describing the latest, cutting-edge methodology, as well as contributions from data professionals in industry who can explain how data science has managed to offer insights into important problems. Data science is a broad church and I want to ensure that the full array of work in this area is represented on this site. I think the diversity of the editorial board will help promote this objective.</p>
<p><strong>Juhi Gupta</strong></p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-10-18-meet-the-team/images/Juhi.png" class="img-fluid"></p>
</div></div><p>I am a lecturer in health data sciences and the deputy programme director of the health data science MSc in the School of Health Sciences, University of Manchester (UoM). I have a background in genetics, pharmacology and bioinformatics, and my doctoral thesis focussed on multi-omics data analysis using machine learning methods for precision medicine. I have worked with scientists, clinical academics and technologists to produce translational research. I am currently investigating adverse health outcomes in people with common diseases using electronic health record data, and I also teach on the health informatics MSc joint programme with UoM and UCL.</p>
<p>I would like to see this platform encourage collaborations and the sharing of ideas and good practice across different disciplines that apply data science skills in their work (or as a hobby). I would like to support budding data scientists to gain useful advice and guidance for upskilling as well as application in real-world situations involving health data and biological data.</p>
<p><strong>Hollie Johnson</strong></p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-10-18-meet-the-team/images/Hollie.png" class="img-fluid"></p>
</div></div><p>I am a data scientist at the National Innovation Centre for Data (NICD), based in Newcastle upon Tyne. Following my undergraduate degree in mathematics, I worked as a software developer both in industry and as a technical research assistant in academia. I later joined the Centre for Doctoral Training in Cloud Computing for Big Data at Newcastle and obtained a PhD in topological data analysis in 2020. Now at the NICD, I specialise in transferring statistics and machine learning skills into industry, through collaborative data science projects.</p>
<p>I am excited to be a member of the editorial board and look forward to seeing Real World Data Science develop into a valuable source of information for aspiring data scientists and professionals alike. I would particularly encourage submissions that demonstrate the use of data science in SMEs and the non-profit sector, as well as perspectives from those with non-standard backgrounds.</p>
<p><strong>Harvey Lewis</strong></p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-10-18-meet-the-team/images/Harvey.png" class="img-fluid"></p>
</div></div><p>I am a senior technology leader, with a diverse background spanning rocket science, data science and research. I have 30 years of experience in artificial intelligence and other emerging technologies and am currently pioneering the use of AI in Ernst &amp; Young’s tax and law practice. I’m a former member of the Open Data User Group, the Public Sector Transparency Board and the Advisory Committee to the All-Party Parliamentary Group on AI. I am a member of techUK’s leadership committee for data analytics and AI, and an honorary senior visiting fellow at The Bayes Business School in London.</p>
<p>I’m passionate about data science but I’m also a fierce advocate for human skills, which are as often underrated as AI is over-hyped. As a member of the editorial board, I’m keen to explore the interplay between artificial and human intelligence in businesses. I’m going to encourage all data scientists to think about the fundamentally human aspects of their work, such as trust and safety, so that we maintain perspective and proportionality in the face of ever-more sophisticated technology.</p>
<p><strong>Detlef Nauck</strong></p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-10-18-meet-the-team/images/Detlef.png" class="img-fluid"></p>
</div></div><p>I am a BT Distinguished Engineer and the head of AI and data science research for BT’s Applied Research Division located at Adastral Park, Ipswich, UK. I have over 30 years of experience in AI and machine learning and lead a programme spanning the work of a large team of international researchers who develop capabilities underpinning future AI systems. A key part of this work is to establish best practices in data science and machine learning, leading to the deployment of responsible and auditable AI solutions that are driving real business value.</p>
<p>I am a computer scientist by training and hold a PhD and a Postdoctoral Degree (Habilitation) in machine learning and data analytics. I am also a visiting professor at Bournemouth University and a private docent at the Otto-von-Guericke University of Magdeburg, Germany. I have published 3 books and over 120 papers, and I hold 15 patents and have 30 active patent applications.</p>
<p>I am passionate about promoting best practice in data science and believe that in the UK the RSS is the ideal professional body to pursue this goal. For me, Real World Data Science is an opportunity to share my experience and inspire a new generation of data scientists.</p>
<p><strong>Fatemeh Torabi</strong></p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-10-18-meet-the-team/images/Fatemeh.png" class="img-fluid"></p>
</div></div><p>I am a research officer and data scientist at Health Data Research UK and a fellow of the RSS. My background is in mathematical statistics and health data science, and my research interests span novel analytical and computational methods for statistical inference in panel data and population health. I am supporting the development of the Real World Data Science platform in the context of health with a specific focus on how health data can be harnessed through data linkage and analysis to answer important questions and improve the lives of our population.</p>
<p><strong>Isabel Sassoon</strong></p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-10-18-meet-the-team/images/Isabel.png" class="img-fluid"></p>
</div></div><p>I am a senior lecturer in computer science (data science) at Brunel University and the programme lead for the data science and analytics MSc programme. My research interests are in data-driven automated reasoning and its transparency and explainability, which brings in data science and artificial intelligence with applications within the health space. I am also championing open science and reproducible analysis in both my research and teaching. I have a PhD in informatics from King’s College London and it was on the topic related to the use of AI to support statistical model selection. Prior to Brunel I was a teaching fellow and research associate at King’s College London and before that I worked for more than 10 years as a data science consultant in industry, including 8 years at SAS UK. </p>
<p>I have been working, researching, consulting and teaching in the data science space for a while and I am passionate about the domain and its applications. I am always interested in sharing and hearing what else is being done to support, inform and inspire all those studying and working in the field of data science. I look forward to sharing case studies, how-to guides and data science profiles through this website.</p>
<p><strong>Christopher Thiele</strong></p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-10-18-meet-the-team/images/Christopher.png" class="img-fluid"></p>
</div></div><p>I’m a principal data scientist at Uniper where I lead a team that focuses mainly on financial business processes and upskilling initiatives. We run projects end to end: from use case ideation, requirement collection and translation to prototyping, assessment, deployment and maintenance. Besides the core analytical and data engineering duties, overarching topics such as data design, data governance and data strategy predominate my days. In my previous role, at the German Economic Institute, I contributed to the development of a cross-functional department that helps apply data science methods in economic research. Projects often involve geospatial analyses or natural language processing. Before that, I worked as a data scientist in customer and marketing analytics, doing statistical analyses such as marketing mix modelling. I have a master’s degree in statistics from Warwick University, a bachelor’s degree in economics from the University of Cologne and I’m trained as an assistant tax consultant in Germany.</p>
<p>I see data science as a creative way to solve problems using software engineering and quantitative modelling techniques and I like to build software pieces that people can interact with. I think that there still exists a lot of confusion about data science as a discipline. Reducing it would promote the realisation of its potential for individuals, as a profession, and our society, as a form of digitalisation. I hope that with Real World Data Science we can provide guidance and clarification to everybody engaged or interested in the field and accompany this young profession’s development.</p>


</section>

 ]]></description>
  <category>People</category>
  <category>Biographies</category>
  <guid>https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-10-18-meet-the-team/meet-the-team.html</guid>
  <pubDate>Tue, 18 Oct 2022 00:00:00 GMT</pubDate>
  <media:content url="https://realworlddatascience.net/news-and-views/editors-blog/posts/2022-10-18-meet-the-team/images/team.png" medium="image" type="image/png" height="64" width="144"/>
</item>
</channel>
</rss>
